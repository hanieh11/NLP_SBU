{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHHgP36S5mT1"
   },
   "source": [
    "#**Embeddings**\n",
    "\n",
    "Hello everybody,\n",
    "in this part we want to show you how to build an embedding matrix and how to use it your projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHRq-USd3iNy",
    "outputId": "c05062e4-1e5d-4643-a54d-92f7915102bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.8.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2MB 137kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3) (1.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.3) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (3.0.4)\n",
      "Installing collected packages: gensim\n",
      "  Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "Successfully installed gensim-3.8.3\n",
      "Collecting hazm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 8.6MB/s \n",
      "\u001b[?25hCollecting nltk==3.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 9.8MB/s \n",
      "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 34.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
      "Building wheels for collected packages: nltk, libwapiti\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394469 sha256=a12a7701d146855b47843074dacca66ae35b4fbdbca4c6128f859142b7502a8f\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154368 sha256=7b3ab52bffdffcb96f8669f68c2c16eee0922ed1cdf360dcf984d6699db95e49\n",
      "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
      "Successfully built nltk libwapiti\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###### array modules ######\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "###### basic modules ######\n",
    "from collections import Counter\n",
    "import tqdm, os, gc, subprocess, zipfile, gdown, pickle\n",
    "#!pip install gdown\n",
    "from pathlib import Path\n",
    "from termcolor import colored\n",
    "\n",
    "###### tensorflow modules ######\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "## Embedding Visualization Modules\n",
    "from tensorboard.plugins import projector\n",
    "import tensorboard\n",
    "\n",
    "###### embedding creation modules ######\n",
    "!pip install gensim==3.8.3\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "###### Text Processing Tools Modules ######\n",
    "!pip install hazm\n",
    "import spacy\n",
    "from hazm import *\n",
    "# in the future you can import step_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqh_mDJgZgNK",
    "outputId": "25d36966-c537-45b7-efc2-d35d91310967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text            :  1 2 3 .به نام خدا. امروز کلاس پردازش زبان طبیعي داریم. من به شما خیر مقدم و تبریک میگم\n",
      "normalizing text    :  ۱ ۲ ۳. به نام خدا. امروز کلاس پردازش زبان طبیعی داریم. من به شما خیر مقدم و تبریک میگم\n",
      "sentence tokenizer  :  ['1 2 3 .به نام خدا.', 'امروز کلاس پردازش زبان طبیعي داریم.', 'من به شما خیر مقدم و تبریک میگم']\n",
      "tokenizing text     :  ['1', '2', '3', '.', 'به', 'نام', 'خدا', '.', 'امروز', 'کلاس', 'پردازش', 'زبان', 'طبیعي', 'داریم', '.', 'من', 'به', 'شما', 'خیر', 'مقدم', 'و', 'تبریک', 'میگم']\n",
      "removing stop words :  ['1', '2', '3', '.به', 'نام', 'خدا.', 'امروز', 'کلاس', 'پردازش', 'زبان', 'طبیعي', 'داریم.', 'خیر', 'مقدم', 'تبریک', 'میگم']\n",
      "removing stop words2:  ['1', '2', '3', '.', 'نام', 'خدا', '.', 'امروز', 'کلاس', 'پردازش', 'زبان', 'طبیعي', '.', 'خیر', 'مقدم', 'تبریک', 'میگم']\n",
      "lemmatizer          :  ['1', '2', '3', '.به', 'نام', 'خدا.', 'امروز', 'کلاس', 'پردازش', 'زبان', 'طبیعي', 'داریم.', 'من', 'به', 'شما', 'خیر', 'مقدم', 'و', 'تبریک', 'میگ']\n",
      "stemmer             :  ['1', '2', '3', '.به', 'نا', 'خدا.', 'امروز', 'کلاس', 'پرداز', 'زب', 'طبیعي', 'داریم.', 'من', 'به', 'شما', 'خیر', 'مقد', 'و', 'تبریک', 'میگ']\n",
      "norm lemmatizer     :  ['۱', '۲', '۳.', 'به', 'نام', 'خدا', '.', 'امروز', 'کلاس', 'پردازش', 'زبان', 'طبیعی', 'داشت#دار', '.', 'من', 'به', 'شما', 'خیر', 'مقدم', 'و', 'تبریک', 'میگ']\n",
      "norm stemmer        :  ['۱', '۲', '۳.', 'به', 'نا', 'خدا', '.', 'امروز', 'کلاس', 'پرداز', 'زب', 'طبیع', 'دار', '.', 'من', 'به', 'شما', 'خیر', 'مقد', 'و', 'تبریک', 'میگ']\n"
     ]
    }
   ],
   "source": [
    "## modules initialization ##\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "sent_tokenizer = SentenceTokenizer()\n",
    "normalizer = Normalizer()\n",
    "tokenizer = WordTokenizer()\n",
    "stop_w_list = stopwords_list()\n",
    "\n",
    "sample_string = '1 2 3 .به نام خدا. امروز کلاس پردازش زبان طبیعي داریم. من به شما خیر مقدم و تبریک میگم'\n",
    "print('raw text            : ', sample_string)\n",
    "print('normalizing text    : ', normalizer.normalize(sample_string))\n",
    "print('sentence tokenizer  : ', sent_tokenizer.tokenize(sample_string))\n",
    "print('tokenizing text     : ', tokenizer.tokenize(sample_string))\n",
    "\n",
    "print('removing stop words : ', [word for word in sample_string.split(sep=' ') if word not in stop_w_list])\n",
    "print('removing stop words2: ', [word for word in tokenizer.tokenize(sample_string) if word not in stop_w_list])\n",
    "\n",
    "print('lemmatizer          : ', [lemmatizer.lemmatize(word) for word in sample_string.split(sep=' ')])\n",
    "print('stemmer             : ', [stemmer.stem(word) for word in sample_string.split(sep=' ')])\n",
    "\n",
    "print('norm lemmatizer     : ', [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(normalizer.normalize(sample_string))])\n",
    "print('norm stemmer        : ', [stemmer.stem(word) for word in tokenizer.tokenize(normalizer.normalize(sample_string))])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptq1OXK52ccD"
   },
   "source": [
    "## **Load and Preprocessing data**\n",
    "\n",
    "The examples here are from [This link](https://colab.research.google.com/drive/16HDMoHsdP9GKRSOYCVZ-O9wC2LcNh-Kt?usp=sharing). For more Details you can check this notebook in future.\n",
    "#### **Mounting Google Drive locally**\n",
    "This is how to mount your Google Drive on your runtime using an authorization code, and how to write and read files there, supporting read, write, and moving files.\n",
    "\n",
    "Note: When using the 'Mount Drive' button in the file browser, no authentication codes are necessary for notebooks that have only been edited by the current user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_snJJRRprNr",
    "outputId": "758a09bf-af9b-434f-e2c3-735920b7f384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3MPtyLhMifV"
   },
   "outputs": [],
   "source": [
    "def un_zip(path):\n",
    "    if not os.path.exists(path):\n",
    "      with zipfile.ZipFile(path+'.zip','r') as zip_ref:\n",
    "          zip_ref.extractall(path)\n",
    "\n",
    "def read_pickle(path):\n",
    "  with open(path, 'rb') as in_file:\n",
    "      data = pickle.load(in_file)\n",
    "  return data\n",
    "\n",
    "def load_csv(path):\n",
    "  df = pd.read_csv(path)\n",
    "  return df\n",
    "\n",
    "def preprocessing_data(line, remove_stop_word=False,\n",
    "                     lemmatize=False,\n",
    "                     stem=False):\n",
    "    line = normalizer.normalize(line)\n",
    "    line = tokenizer.tokenize(line)\n",
    "    if remove_stop_word:\n",
    "        line = [word for word in line if word not in stop_w_list]\n",
    "    if stem:\n",
    "        line = [stemmer.stem(word) for word in line]\n",
    "    if lemmatize:\n",
    "        line = [lemmatizer.lemmatize(word) for word in line]\n",
    "        line\n",
    "    line = ' '.join(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USl34mVD2kAJ"
   },
   "source": [
    "## **Create Embedding**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk74qnQn3TfO"
   },
   "source": [
    "#### **word embedding**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyHGFEvdPMOO"
   },
   "outputs": [],
   "source": [
    "def create_embeddings(sentences,embedding_path='defualt_path.gensimmodel', word_vec_type='fasttext',\n",
    "                      **params):\n",
    "    if word_vec_type == 'fasttext':\n",
    "        model = FastText(sentences, **params)\n",
    "    else:\n",
    "        model = Word2Vec(sentences, **params)\n",
    "    print('Saving the embedding models')\n",
    "    model.wv.save_word2vec_format(embedding_path)\n",
    "    return model\n",
    "\n",
    "def fasttext(sentence_text,embedding_path,embedding_dim,MYSG,iteration):\n",
    "   return create_embeddings(sentence_text,embedding_path=embedding_path + '.gensimmodel' ,\n",
    "                            size=embedding_dim, word_vec_type='fasttext',\n",
    "                            sg=MYSG, workers=5,\n",
    "                            iter=iteration,min_count=20,\n",
    "                            word_ngrams=1, min_n=2, max_n=8, bucket=2000000,)\n",
    "\n",
    "def w2v(sentence_text,embedding_path,embedding_dim,MYSG,iteration):\n",
    "        return create_embeddings(sentence_text,embedding_path=embedding_path + '.gensimmodel',\n",
    "                                  size=embedding_dim, word_vec_type='w2v', window=10,\n",
    "                                  sg=MYSG, workers=5,min_count=20,\n",
    "                                  iter=iteration,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdMXn1D4iY9L"
   },
   "source": [
    "#### **Test Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxfToqkjiauB"
   },
   "outputs": [],
   "source": [
    "def test_most_similar_word_list(model,word_list,top_number=3):\n",
    "  in_words_list = [word for word in word_list if word in model.wv.vocab.keys()]\n",
    "  if len(in_words_list) == 0 :\n",
    "    return\n",
    "  word_not_in_vocab = [word for word in word_list if word not in in_words_list]\n",
    "  print('these words are not in vocab',word_not_in_vocab)\n",
    "\n",
    "  most_similar = [related_word[0] for related_word in model.wv.most_similar(positive=[word for word in in_words_list], topn=top_number)]\n",
    "  print('most similar words to word_list : ',most_similar)\n",
    "  most_similar_cosmul = [related_word[0] for related_word in model.wv.most_similar_cosmul(positive=[word for word in in_words_list], topn=top_number)]\n",
    "  print('most similar cosmul words to word_list : ' , most_similar_cosmul)\n",
    "\n",
    "def test_most_similar_word(model,word,top_number=3) :\n",
    "  if word not in model.wv.vocab.keys():\n",
    "    print('word is not in vocab')\n",
    "    return\n",
    "  print('most similar words : ',[related_word[0] for related_word in model.wv.similar_by_word(word=word, topn=top_number)])\n",
    "\n",
    "def test_most_similar_analogy(model, in_word_1, in_word_2, out_word_1, out_word_2, top_number=3):\n",
    "  if in_word_1 not in model.wv.vocab.keys():\n",
    "    # print('first word is not in vocab')\n",
    "    return -1\n",
    "  if in_word_2 not in model.wv.vocab.keys():\n",
    "    # print('second word is not in vocab')\n",
    "    return -1\n",
    "  if out_word_1 not in model.wv.vocab.keys():\n",
    "    # print('third word is not in vocab')\n",
    "    return -1\n",
    "\n",
    "  in_1 = model.wv.vectors[model.wv.vocab[in_word_1].index]\n",
    "  in_1_2 = in_1 / sum(in_1 ** 2) ** 0.5\n",
    "\n",
    "  in_2 = model.wv.vectors[model.wv.vocab[in_word_2].index]\n",
    "  in_2_2 = in_2 / sum(in_2 ** 2) ** 0.5\n",
    "\n",
    "  out_1 = model.wv.vectors[model.wv.vocab[out_word_1].index]\n",
    "  out_1_2 = out_1 / sum(out_1 ** 2) ** 0.5\n",
    "\n",
    "  out_2_2 = in_2 + out_1 - in_1\n",
    "  out_2_3 = in_2_2 + out_1_2 - in_1_2\n",
    "\n",
    "  distance_by_vec = model.wv.similar_by_vector(out_2_2, topn=top_number)\n",
    "\n",
    "  normalized_distance_by_vec = model.wv.similar_by_vector(out_2_3, topn=top_number)\n",
    "\n",
    "  most_similar_distance = model.wv.most_similar(positive=[in_word_2, out_word_1], \\\n",
    "                                    negative=[in_word_1], topn=top_number)\n",
    "\n",
    "  most_similar_cosmul_distance = model.wv.most_similar_cosmul(positive=[in_word_2, out_word_1],\\\n",
    "                                    negative=[in_word_1], topn=top_number)\n",
    "\n",
    "  distance_by_vec = [related_word[0] for related_word in distance_by_vec]\n",
    "  normalized_distance_by_vec = [related_word[0] for related_word in normalized_distance_by_vec]\n",
    "  most_similar_distance = [related_word[0] for related_word in most_similar_distance]\n",
    "  most_similar_cosmul_distance = [related_word[0] for related_word in most_similar_cosmul_distance]\n",
    "\n",
    "  if out_word_2 in distance_by_vec:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANvKJRBbO-mB"
   },
   "source": [
    "## Visualizing Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXGYA_Y6PDHR"
   },
   "outputs": [],
   "source": [
    "def visualize_embedding(model, log_dir):\n",
    "  Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "  with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "      for subwords in model.wv.index2word:\n",
    "          f.write(\"{}\\n\".format(subwords))\n",
    "\n",
    "  # # Fill in the rest of the labels with \"unknown\"\n",
    "  # for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
    "  #   f.write(\"unknown #{}\\n\".format(unknown))\n",
    "  # Save the weights we want to analyse as a variable. Note that the first\n",
    "  # value represents any unknown word, which is not in the metadata, so\n",
    "  # we will remove that value.\n",
    "  weights = tf.Variable(model.wv.vectors)\n",
    "  # Create a checkpoint from embedding, the filename and key are\n",
    "  # name of the tensor.\n",
    "  checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "  checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "  # Set up config\n",
    "  config = projector.ProjectorConfig()\n",
    "  embedding = config.embeddings.add()\n",
    "  # The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
    "  embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "  embedding.metadata_path = 'metadata.tsv'\n",
    "  projector.visualize_embeddings(log_dir, config)\n",
    "  input('see the result and if you pass press any key for continue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OGxEjhD2zf8"
   },
   "source": [
    "## **Run & Results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I15Wn2881kR4",
    "outputId": "da2c0b7e-52a6-402d-a366-41c372801692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/u/0/uc?id=1HCPSk0NR16yMUA0UvKpDbMQ-TfSevd04&export=download\n",
      "To: /content/drive/MyDrive/MasterPro/Persian_Corpus.txt.zip\n",
      "61.7MB [00:00, 99.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/content/drive/MyDrive/MasterPro/\"\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = 'https://drive.google.com/u/0/uc?id=1HCPSk0NR16yMUA0UvKpDbMQ-TfSevd04&export=download'\n",
    "\n",
    "output = save_path + 'Persian_Corpus.txt.zip'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "un_zip(save_path + 'Persian_Corpus.txt')\n",
    "with open (save_path + 'Persian_Corpus.txt/Persian_Corpus.txt') as file:\n",
    "  data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWVtAvsgSr1-"
   },
   "outputs": [],
   "source": [
    "data = data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoBPCBbWEYno",
    "outputId": "a5f538fc-e231-4778-ad3c-7343201f54f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40182/2000000 [00:04<03:16, 9996.03it/s] "
     ]
    }
   ],
   "source": [
    "cleaned_data = []\n",
    "for doc in tqdm.tqdm(data):\n",
    "  cleaned_data.append(preprocessing_data(doc, remove_stop_word=True,lemmatize=False,stem=False))\n",
    "print('\\n')\n",
    "print('cleaned_line len is : ', len(cleaned_data))\n",
    "print(cleaned_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6S5Py23BGxHL",
    "outputId": "8572f047-04f4-4d7f-88b5-69aab86519d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بسم', 'الله', 'الرحمن', 'الرحیم']\n",
      "Saving the embedding models\n"
     ]
    }
   ],
   "source": [
    "splited_line = [line.split(sep=' ') for line in cleaned_data if len(line.split(sep=' ')) > 3]\n",
    "print(splited_line[0])\n",
    "### for skipgram use 1 and for cbow use 0\n",
    "MYSG = 0\n",
    "#################### w2v #####################\n",
    "save_name = 'w2v-50-10-12'\n",
    "embedding_path = save_path + save_name\n",
    "w2v_model = w2v(splited_line,embedding_path=embedding_path,embedding_dim = 50,MYSG=MYSG,iteration=12)\n",
    "#########################################\n",
    "# save_name = 'fasttext-100-1'\n",
    "# embedding_path = save_path + save_name\n",
    "# fastext_model = fasttext(splited_line,embedding_path=embedding_path,embedding_dim = 100,MYSG=MYSG,iteration=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGqMmh6fvQ2r",
    "outputId": "f59162e9-4947-4d29-8112-bec70b484e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see the result and if you pass press any key for continuee\n"
     ]
    }
   ],
   "source": [
    "visualize_embedding(w2v_model, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmRAyLAWasY8"
   },
   "outputs": [],
   "source": [
    "log_dir = save_path + 'tensorboard/logs/' + 'w2v' + '/'\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNimbC_LbIv_"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "path = '/content/drive/MyDrive/'\n",
    "embeddings = ['fasttext-100-0.gensimmodel', 'fasttext-100-1.gensimmodel', 'fasttext-50-0.gensimmodel', 'fasttext-50-1.gensimmodel', 'w2v-100-10-12.gensimmodel','w2v-100-5-12.gensimmodel', 'w2v-50-10-12.gensimmodel', 'w2v-50-5-12.gensimmodel']\n",
    "\n",
    "for model_conf in embeddings:\n",
    "    embedding_path = path + model_conf\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path)\n",
    "\n",
    "    p = [0, 0, 0, 0]\n",
    "    for i, top_n in enumerate([1, 5, 10, 20]):\n",
    "        with open('Test_file.txt') as test:\n",
    "            corrects = 0\n",
    "            counts = 0\n",
    "            for line in test.readlines():\n",
    "                in_word_1, in_word_2, out_word_1, out_word_2 = line[:-1].split(',')[1:]\n",
    "                res = test_most_similar_analogy(model, in_word_1, in_word_2,\n",
    "                                                out_word_1, out_word_2, top_number=top_n)\n",
    "                if res != -1:\n",
    "                    corrects += res\n",
    "                    counts += 1\n",
    "\n",
    "        p[i] = corrects / counts\n",
    "    print(model_conf)\n",
    "    results.append({'model_type': model_conf,\n",
    "                    'Top-1': p[0],\n",
    "                    'Top-5': p[1],\n",
    "                    'Top-10': p[2],\n",
    "                    'Top-20': p[3]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otmhGfYNBn9E"
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "  print(results[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
